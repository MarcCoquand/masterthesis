\chapter{Background}\label{background}

This chapter will look at how concerns and requirements made impact the
maintainability and testability. It aims to establish what are the current
methods of developing applications and how software engineers today approach
testing. 

\section{Architecture} 

When developing large scale applications, often the requirements are as follows:

\begin{itemize}
    \item There is a team of developers
    \item New team members must get productive quickly
    \item The system must be continuously developed and adapt to new
        requirements
    \item The system needs to be continuously tested
    \item System must be able to adapt to new and emerging frameworks
\end{itemize}

Two different approaches to developing these large scale applications are
microservice and monolithic systems. The monolithic system comprises of one big
``top-down'' architecture that dictates what the program should do. This is
simple to develop using some IDE and deploying simply requires deploying some
files to the runtime. 

As the system starts to grow the large monolithic system becomes harder to
understand as the size doubles. As a result, development typically slows down.
Since there are no boundaries, modularity tends to break down and the IDE
becomes slower over time, making it harder to replace parts as needed. Since
redeploying requires the entire application to be replaced and tests becomes
slower; the developer becomes less productive as a result. Since all code is
written in the same environment introducing new technology becomes harder.

Enter microservices, in a microservice architecture the program comprises of
small entities that each have their own responsibility. There can be one service
for metrics, one that interacts with the database and one that takes care of
frontend. This decomposition allows the developers to easier understand parts of
the system, scale into autonomous teams, IDE becomes faster since codebases are
smaller, faults become easier to understand as they each break in isolation.
Also long-term commitment to one stack becomes less and it becomes easier to
introduce a new stack. 

The issue with microservices is that when scaling the complexity becomes harder
to predict. While testing one system in isolation is easier testing the entire
system with all parts together becomes harder. 

\subsection{Unit testing}

Unit testing is a testing method where the individual units of code and
operating procedures are tested to see if they are fit for use. A unit is
informally the smallest testable part of the application. To deal with units
dependence one can use method stubs, mock objects and fakes to test in
isolation. The goal of unit testing is to isolate each part of the programs and
ensure that the individual parts are correct. It also allows for easier
refactoring since it ensures that the individual parts still satisfy their part
of the application.

To create effective unit tests it's important that it's easy to mock examples.
This is usually hindered if the code is dependant on some state since previous
states might affect future states.

\subsection{Property-based testing}

Property-based testing tests the properties that a function should fulfill. A
property is some logical condition that a specification defines the function
should fulfill.  Compared to unit testing where the programmer creates the mock
values; property based testing generates values automatically to find a
contradiction. For example a function $reverse\, : \; [a]\, \rightarrow\, [a]$,
which takes a list and reverses it's items, should have the property
$reverse\circ reverse\; x = x$. A unit test would check that $reverse [1,2,3] =
[3,2,1]$; a property-based test would instead generate a list of values randomly
and check it's properties hold.

\subsection{Integration testing}

Whereas unit testing validates that the individual parts work in isolation;
integration tests make sure that the modules work when combined. The purpose is
to expose faults that occurs when the modules interact with each other.

\subsection{End-2-End Tests}

An End-2-End test (also known as E2E test) is a test that tests an entire
passage through the program, testing multiple components on the way. This
sometimes requires setting up an emulated environment mock environment with fake
variables.

\subsection{Challenges}

No clear guidelines exists on how many tests should be written. Google
advocates for a 10\% E2E-tests, 20\% integration tests and the rest to be unit
tests.~\cite{googleontests} Spotify advocates instead to have a low amount of
unit tests and instead focus on integration tests and having 80\% integration
tests, 10\% E2E-tests, 10\% unit tests.~\cite{spotifyontests}

When writing unit tests that depend on some environment, for example fetching a
user from some database, it can be difficult to test without simulating the
environment itself. In such cases one can use dependency injections and mock the
environment with fake data. Dependency injection is a method that substitutes
environment calls and returns data instead. The issue with unit tests is that
even if a feature works well in isolation it does not imply that it will work
well when composed with other functions.

The challenge in integration and E2E-tests comes with simulating the entire
environments. Given a server connected to some file storage and a database it
requires setting up a local simulation of that environment to run the tests.
This results in slower execution time for tests and also requires work setting
up the environment. Thus it ends up being costly. Also the bigger the space
that is being tested the less close the test is to actually finding the error,
thus the test ends up finding some error but it can be hard to track it down.

\subsubsection{Readability}

The hypothesis of this thesis is that after designing a system to be testable
the code becomes much harder to understand and more complex. The thesis
investigates if the different language paradigms then end up having vastly
different complexity and being much less readable after creating testable
systems and if paradigms factor into that complexity. 

\section{Complexity and relation to testability}

To establish how to look at complexity and readability, there exists different
metrics for measuring software quality. Lines of Code (LOC) can be used as a
measure in the software to find defects.~\cite{defectloc} As this study will
measure different programming languages and software paradigms where syntax is
vastly different, it follows that using LOC to measure fault is flawed. Thus,
here are some other ways to measure complexity:

\begin{description}

    \item [Cyclomatic Complexity] Described further in Chapter~\ref{theory}

    \item [Halsteads metric] A metric that relates to the difficulty of writing
    or understanding code related to operators and operands.~\cite{bergklaas}

    \item [Chidamber and Kemerer Metrics~\cite{chidamber}] A complexity measure
    for Object-oriented programs. Since this study compares functional to
    object-oriented progams this metric is ill suited as it does not allow for
    comparative analysis.

\end{description}

Berg Van Der Klaas explored Halsteads metric and compared OOP and functional
programming. The study notes that the psychological complexity needs to be taken
into account and noted that the use higher order functions may affect results
for functional programs for the Halstead metric. To measure complexity of
software, a program could instead be measured at a lexical level. No
quantitative measure was found for measuring the linguistic structures of the
program and the comprehension. However a qualitative measure called Cognitive
Dimensions was found will be used instead.  Cognitive Dimensions inspects
fourteen different aspects and they are not always applicable for all projects.
Prior research suggests omitting some which has been taken into account and is
explained further in Chapter~\ref{theory}.~\cite{euguenkiss}

