\chapter{Method}

To evaluate if the functional approach to creating servers is more maintainable
than existing solutions, a comparative study will be done. A popular library for
developing server applications is by using an unopiniated solution using
Express, which is a good candidate to compare to Cause.

Express is an unopiniated server framework written for Node.js for Javascript.
That a framework is unopiniated means that it does not force you to architecture
your code in any specific way.  

\section{Constructing the server in Express and Cause}

To measure the maintainability, a comparison will be made by comparing a correct
construction of an idiomatic server both made in Cause and the popular framework
for Node Express. They will feature similar functionality which is a library api
with the endpoints:

\begin{itemize}
    \item \texttt{GET ``api/books?released=int\&author=string''} Get a list of
    books and optionally ask for a specific author or a book from a specific
    year
    \item \texttt{DELETE ``api/books/:id''} Delete a book with a specified ID.
    \item \texttt{POST ``api/books/:id'' OR ``api/books/''} Create a new book or
    override a specific book
\end{itemize}

The server will also make use of a hashmap for database connection.

The accepted content types will be \texttt{application/json} and
\texttt{www-url-formencoded} for all endpoints and the displayable content-types
are \texttt{text/plain} and \texttt{application/json}. Both implementations will handle
all of the error cases. They will also be written in an idiomatic way, that is
they will not take the challenges outlined in Chapter~\ref{background} into
consideration; the only requirement is that they compile.

The two solutions are in the appendix, with the ReasonML implementation in
section~\ref{reasonmlrest} and the Node solution in section~\ref{nodejsrest}

\section{Evaluating maintainability}

The aspects that to be evaluated when measuring maintainability were discussed
in Chapter~\ref{background}. This study will focus on evaluating the following
criteria:

\begin{itemize}
    \item Testability
    \item Extendability
    \item Readability
    \item Error-proneness
\end{itemize}

Chapter~\ref{theory} established the SOLID guidelines in Functional programming
as guidelines for maintainable software. However these guidelines do not state
anything about the readability of the software. Thus two different methods will
be needed to be used to evaluate the readability and to measure the testability,
extendability and error-proneness.

To evaluate the testability, error-proneness and extendability, a comparison
between the two solutions, one in ReasonML and the other in Nodejs, and it's
adherence to the SOLID principle can be done since SOLID was translated to
Functional programming in Chapter~\ref{theory}, where deductive reasoning can be
used to find if the solution follows each criteria. 

\subsection{Evaluating readability through code reviews}

Code reviews, also known as peer reviews, is an activity where a human evaluates
the program to check for defects, finding better solutions and find readability
aspects. 

To measure the readability of the REST library, a semi-structured code review is
conducted on five different people with varying knowledge of REST apis and
functional programming.

\subsubsection{Semi-structured interviews}

Semi-structured interviews diverges from a structured interview which has a set
amount of questions. In a semi-structured interview the interview is open and
allows for new ideas to enter the discussion. 

Semi-structured interviews are used to gather focused qualitative data. It is
useful for finding specific insights in regards to the readability of the code
and provides insights as to the code can actually be understood by the general
user.

To conduct an semi-structured interview, the interview should avoid leading
questions and use open-ended questions to get descriptive answers rather than
yes or no answers. 

The questions that will be asked are presented below.

\begin{itemize}
    \item What is your experience with RESTful APIs?
    \item What is your experience with Express?
    \item What is your experience with ReasonML?
    \item After being presented the code api, can you explain what it does?
    \item Which media types does the endpoint post accept?
    \item What is the uri of DELETE
    \item Which media types representations can the endpoint show?
    \item Can you demonstrate how you would extend the api and add a new endpoint
    for a PUT request.
    \item Looking at the javascript api, can you explain what it does?
    \item Which media types does the endpoint get accept?
    \item Which content type and accept does post have?
\end{itemize}

\subsection{Evaluating the answers}

After performing the interviews conclusions can be made by interpreting the
answers to conclude if the code is readable or not. If the code is readable the
users being interviewed should be able to explain to the author what the code
does.

In order to reduce the bias in the experiments each user will be shown a
different code base first. So the 3 users will be shown the implementation in
ReasonML and 2 users will be shown the implementation in Express. This also
shows gives insights as to the reason the users do not understand the solution.

So in summary, the way each aspect of maintainability will be evaluated in both
solutions by the following:

\begin{description}
    \item [Testability] Evaulated by comparing the number of dependencies that
    need to be mocked. 
    \item [Extendability] Evaluated by comparing to SOLID principles.
    \item [Readability] Evaluated by comparing to SOLID principles.
    \item [Error-proneness] Evaluated by SOLID principles and the interviews
        where we ask to extend the solution with a PUT request.
\end{description}

Afterwards from there a discussion can be had about the strengths and weaknesses
of both solutions and the impacts of maintainability by using functional
programming for developing REST servers.





