\chapter{Method}

To evalute if the functional approach to creating servers is more maintainable
than existing solutions, a comparative study will be done. A popular library for
developing server applications is by using an unopiniated solution using
Express, which is a good candidate to compare to Cause.

Express is an unopiniated server framework written for Node.js for Javascript.
That a framework is unopiniated means that it does not force you to architecture
your code in any specific way.  

\section{Constructing the server in Express and Cause}

To measure the maintainability, a comparison will be made by comparing a correct
construction of an idiomatic server both made in Cause and the popular framework
for Node Express. They will feature similar functionality which is a library api
with the endpoints:

\begin{itemize}
    \item \texttt{GET ``api/books?released=int\&author=string''} Get a list of
    books and optionally ask for a specific author or a book from a specific
    year
    \item \texttt{DELETE ``api/books/:id''} Delete a book with a specified ID.
    \item \texttt{POST ``api/books/:id'' OR ``api/books/''} Create a new book or
    override a specific book
\end{itemize}

The server will also make use of a hashmap for database connection.

The accepted content types will be \texttt{application/json} and
\texttt{www-url-formencoded} for all endpoints and the displayable content-types
are \texttt{text/plain} and \texttt{application/json}. Both implementations will handle
all of the error cases. They will also be written in an idiomatic way, that is
they will not take the challenges outlined in Chapter~\ref{background} into
consideration; the only requirement is that they compile.

\section{Evaluating maintainability}

The aspects that to be evaluated when measuring maintainability were discussed
in Chapter~\ref{background}. This study will focus on evaluating the following
criterias:

\begin{itemize}
    \item Testability
    \item Extendability
    \item Readability
    \item Error-proneness
\end{itemize}

Each of them require a different approach for evaluation.

\subsection{Evaluating testability}

The testability of the code is determined by it's aherence to SOLID principles,
in particular of it's use of inversion of control. If a solution has less
dependencies, it becomes easier to use unit tests to test the code and less
resources are needed to create integration tests and E2E-tests.

Evaluation can be done then by counting the amount of mocked dependencies in the
imperative solution and the functional solution.

\subsection{Evaluating error-proneness and extendability}

To evaluate error-proneness and extendability an expert analysis will be used.
Cognitive Dimensions is a framework for evaluating the usability of programming
languages and to find areas of improvements.~\cite{GREEN1996131} It allows us to
evaluate the quality of a design and explore what future improvements could be
made. As part of the Cognitive Dimensions, 14 different Cognitive Dimensions of
Notation exist. A notation depends on the specific context, in this case the
notation is the languages themselves and their architecture. The author of the
framework recommends omitting the dimensions that are not applicable to the
notation. This framework is used to evaluate the safety, error-proneness and
extendability of both the Express and Cause solution.

\begin{description}

\item[ Viscosity ]

How much work does it take to make small changes? How easy is the code to
refactor? If small changes requires consequent adjustments then that is a
problem. As a viscous system cause a lot more work for the user and break the
line of thought.

\item[ Visibility ]

How easy is it to navigate the source code to find the parts that you want?

\item[ Hidden dependencies ]

Are there hidden dependencies in the source code. Does a change in one part of
the source code lead to unexpected consequences in another part of the code.
Every dependency that matters to the user should be accessible in both
directions. 

\item[ Role-expressiveness ]

How obvious is each sub-component of the source code to the solution as a whole?

\item[ Abstraction ]

What are the levels of abstraction in the source code? Can the details be
encapsulated?

\item[ Secondary notation ]

Are there any extra information being conveyed to the user in the source code?

\item[ Closeness of mapping ]

By looking at the source code, how close do we find it to be to the case
we are solving?

\item[ Consistency ]

How much of the rest can the user guess 

\item[ Diffuseness or terseness ]

How much space and symbols does the source code need to produce a certain result
or express a meaning?

\item[ Hard mental operations ]

Where does the hard mental processing lie? Is it more when writing the source
code itself rather than solving the case, I.E. the semantic level? Does one
sometimes need to resort to pen and paper to keep track of what is happening?

\item[ Provisionality ]

How easy is it to get feedback of something before you have completed the entire
system?

\item[ Progressive evaluation ]

How obvious the role of each component of the source code in the solution as a
whole?

\item[ Error proneness ]

To what extent does the programming paradigm and language help minimise errors? Are
there any structures or complexities that lead to it being easier to make
errors?
\end{description}

\noindent For this study we will investigate the following dimensions: 

\begin{itemize}
    \item Diffuseness or terseness
    \item Closeness of mapping
    \item Hard Mental Operations
    \item Visibility
    \item Hidden dependencies
    \item Abstraction
    \item Error-proneness 
\end{itemize}

\noindent We omit the other dimensions as related work concluded that the other
dimensions did not bring much weight when evaluating the different
paradigms.~\cite{euguenkiss}

These aspects can also give us insights in the other aspects of mainatainability
and will be used for discussion and evaluation.

\subsection{Evaluating readability through code reviews}

Code reviews, also known as peer reviews, is an activity where a human evaluates
the program to check for defects, finding better solutions and find readability
aspects. 

To measure the readability of the REST library, a semi-structured code review is
conducted on five different people with varying knowledge of REST apis and
functional programming.

\subsubsection{Semi-structured interviews}

Semi-structured interviews diverges from a structured interview which has a set
amount of questions. In a semi-structured interview the interview is open and
allows for new ideas to enter the discussion. 

Semi-structured interviews are used to gather focused qualitative data. It is
useful for finding specific insights in regards to the readability of the code
and provides insights as to wether or not the code can actually be understood by
the general user.

To conduct an semi-structured interview, the interview should avoid leading
questions and use open-ended questions to get descriptive answers rather than
yes or no answers. 

The questions that will be asked are presented below.

\begin{itemize}
    \item What is your experience with Express?
    \item What is your experience with ReasonML?
    \item After being presented the code api, can you explain what it does?
    \item Which media types does the endpoint accept?
    \item Which media types representations can the endpoint show?
    \item Can you demonstrate how you would extend the api and add a new endpoint
    for a PUT request.
\end{itemize}

\subsection{Evaluating the answers}

After performing the interviews conclusions can be made by having the author
interprate the answers to conclude if the code is readable or not. If the code
is readable the users being interviewed should be able to explain to the author
what the code does.

In order to reduce the bias in the experiments each user will be shown a
different code base first. So the 3 users will be shown the implementation in
ReasonML and 2 users will be shown the implementation in Express.

So in summary, the way each aspect of maintainability will be evaluated in both
solutions by the following:

\begin{description}
    \item [Testability] Evaulated by comparing the number of dependencies that
    need to be mocked. 
    \item [Extendability] Evaluated by analysing the results from cognitive
    dimensions and the interviews.
    \item [Readability] Evaluated by merging the results of cognitive dimensions
    and the interviews.
    \item [Error-proneness] Evaluated by analysing the results from cognitive
    dimensions and the interviews.
\end{description}

Afterwards from there a discussion can be had about the strenghts and weaknesses
of both solutions and the impacts of maintainability by using functional
programming for developing REST servers.



